"""
–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ —É—Å–ª—É–≥ –≤ CSV —Å 4-—É—Ä–æ–≤–Ω–µ–≤—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.
–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ CSV:
- service_code: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥ (st1.2.3, ds4.5.6, 123, 1.2.3.4)
- prefix_type: –ø—Ä–∏–∑–Ω–∞–∫ ds, st, simple, dotted (2, 3, 4, 5)
- hierarchy_idx: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∫–æ–¥ –∏–∑ –∫–æ–ª–æ–Ω–∫–∏ A
- global_idx: —Å–∫–≤–æ–∑–Ω–æ–π —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å
- description: –æ–ø–∏—Å–∞–Ω–∏–µ —É—Å–ª—É–≥–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import json
import os
from pathlib import Path

class ServiceXLSXConverter:
    """–ö–æ–Ω–≤–µ—Ä—Ç–µ—Ä XLSX ‚Üí CSV –¥–ª—è —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —É—Å–ª—É–≥"""
    
    def __init__(self, xlsx_path: str):
        self.xlsx_path = xlsx_path
        self.df = None
        self.vocabs = {}
        
    def load_xlsx(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç XLSX —Ñ–∞–π–ª"""
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {self.xlsx_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º XLSX
        self.df = pd.read_excel(self.xlsx_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ['A', 'TEXTCODE']
        missing_cols = [col for col in required_columns if col not in self.df.columns]
        
        if missing_cols:
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_cols}. "
                           f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(self.df.columns)}")
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.df)} —Å—Ç—Ä–æ–∫")
        print(f"üìä –ö–æ–ª–æ–Ω–∫–∏: {list(self.df.columns)}")
        print(f"üîç –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫:")
        print(self.df[['A', 'TEXTCODE']].head())
        
        return self.df
    
    def clean_and_prepare(self) -> pd.DataFrame:
        """–û—á–∏—â–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
        print("\nüßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # 1. –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∫–æ–¥–∞–º–∏
        initial_count = len(self.df)
        self.df = self.df.dropna(subset=['TEXTCODE'])
        print(f"   –£–¥–∞–ª–µ–Ω–æ {initial_count - len(self.df)} —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –∫–æ–¥–∞–º–∏")
        
        # 2. –ü—Ä–∏–≤–æ–¥–∏–º –∫–æ–¥—ã –∫ —Å—Ç—Ä–æ–∫–æ–≤–æ–º—É —Ç–∏–ø—É –∏ –æ—á–∏—â–∞–µ–º
        self.df['TEXTCODE'] = self.df['TEXTCODE'].astype(str).str.strip()
        
        # 3. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É A (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –∫–æ–¥)
        self.df['A'] = pd.to_numeric(self.df['A'], errors='coerce')
        
        # 4. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∫–æ–¥—É —É—Å–ª—É–≥–∏
        self.df = self.df.drop_duplicates(subset=['TEXTCODE'])
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {len(self.df)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–¥–æ–≤")
        
        return self.df
    
    def determine_prefix_type(self, service_code: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –ø—Ä–µ—Ñ–∏–∫—Å–∞ –∫–æ–¥–∞ —É—Å–ª—É–≥–∏"""
        if not service_code or pd.isna(service_code):
            return 'simple'
        
        code_str = str(service_code)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å ds –∏–ª–∏ st
        if code_str.lower().startswith('ds'):
            return 'ds'
        elif code_str.lower().startswith('st'):
            return 'st'
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ç–æ—á–∫–∏ –≤ –∫–æ–¥–µ (–∫—Ä–æ–º–µ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤)
        elif '.' in code_str:
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–æ—á–∫–∏ –∏ –∫–æ–¥ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ —Ü–∏—Ñ—Ä –∏ —Ç–æ—á–µ–∫
            code_without_prefix = code_str.lower().replace('ds', '').replace('st', '')
            if all(c.isdigit() or c == '.' for c in code_without_prefix):
                return 'dotted'
        
        # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏
        return 'simple'
    
    def prefix_type_to_idx(self, prefix_type: str) -> int:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–∏–ø –ø—Ä–µ—Ñ–∏–∫—Å–∞ –≤ —á–∏—Å–ª–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å"""
        mapping = {
            'ds': 2,      # ds –∫–æ–¥—ã
            'st': 3,      # st –∫–æ–¥—ã
            'simple': 4,  # –ø—Ä–æ—Å—Ç—ã–µ –∫–æ–¥—ã –±–µ–∑ —Ç–æ—á–µ–∫
            'dotted': 5   # –∫–æ–¥—ã —Å —Ç–æ—á–∫–∞–º–∏ (1.2.3.4)
        }
        return mapping.get(prefix_type, 4)  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é simple
    
    def build_vocabularies(self) -> Dict[str, Dict]:
        """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print("\nüìù –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π...")
        
        # 1. –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ç–∏–ø–æ–≤ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
        prefix_types = []
        for code in self.df['TEXTCODE']:
            prefix_type = self.determine_prefix_type(code)
            if prefix_type and prefix_type not in prefix_types:
                prefix_types.append(prefix_type)
        
        prefix_types = sorted(prefix_types)
        prefix_to_idx = {'<PAD>': 0, '<UNK>': 1}
        for prefix_type in prefix_types:
            prefix_to_idx[prefix_type] = self.prefix_type_to_idx(prefix_type)
        
        print(f"   –¢–∏–ø–æ–≤ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(prefix_types)} ({', '.join(prefix_types)})")
        
        # 2. –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –∫–æ–¥–æ–≤ (A)
        # –ë–µ—Ä–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ A, —É–±–∏—Ä–∞–µ–º NaN
        a_values = self.df['A'].dropna().unique()
        a_values = sorted([int(a) for a in a_values if not pd.isna(a)])
        
        a_to_idx = {'<PAD>': 0, '<UNK>': 1}
        for idx, a in enumerate(a_values, start=2):
            a_to_idx[a] = idx
        
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –∫–æ–¥–æ–≤ (A): {len(a_values)}")
        
        # 3. –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–ª–Ω—ã—Ö –∫–æ–¥–æ–≤ —É—Å–ª—É–≥ (—Å–∫–≤–æ–∑–Ω–∞—è –Ω—É–º–µ—Ä–∞—Ü–∏—è)
        codes = sorted(self.df['TEXTCODE'].unique())
        code_to_idx = {'<PAD>': 0, '<UNK>': 1}
        for idx, code in enumerate(codes, start=2):
            code_to_idx[code] = idx
        
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–¥–æ–≤ —É—Å–ª—É–≥: {len(codes)}")
        
        self.vocabs = {
            'prefix': prefix_to_idx,
            'hierarchy': a_to_idx,
            'code': code_to_idx
        }
        
        return self.vocabs
    
    def create_output_dataframe(self) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π DataFrame —Å 5 –∫–æ–ª–æ–Ω–∫–∞–º–∏"""
        print("\nüõ†Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ DataFrame...")
        
        output_data = []
        
        for _, row in self.df.iterrows():
            service_code = row['TEXTCODE']
            a_value = row['A']
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ, –µ—Å–ª–∏ –µ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–æ–Ω–∫–∞
            description = row['NAME']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø—Ä–µ—Ñ–∏–∫—Å–∞
            prefix_type = self.determine_prefix_type(service_code)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –∏–∑ —Å–ª–æ–≤–∞—Ä–µ–π
            prefix_idx = self.vocabs['prefix'].get(prefix_type, 1)  # 1 = UNK
            hierarchy_idx = self.vocabs['hierarchy'].get(a_value, 0) if pd.notna(a_value) else 0  # 0 = PAD
            global_idx = self.vocabs['code'].get(service_code, 1)  # 1 = UNK
            
            output_data.append({
                'service_code': service_code,
                'prefix_type': prefix_idx,
                'hierarchy_idx': hierarchy_idx,
                'global_idx': global_idx,
                'description': description
            })
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        output_df = pd.DataFrame(output_data)
        print(f"DDDD: {output_data[['hierarchy_idx']]}")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ global_idx –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        output_df = output_df.sort_values('global_idx')
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω DataFrame —Å {len(output_df)} –∑–∞–ø–∏—Å—è–º–∏")
        print("\nüìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
        print(output_df[['service_code', 'prefix_type', 'hierarchy_idx', 
                        'global_idx', 'description']].head(10))
        
        return output_df
    
    def save_output_csv(self, output_df: pd.DataFrame, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç DataFrame –≤ CSV"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV –≤ {output_path}")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ 5 –∫–æ–ª–æ–Ω–æ–∫
        output_df[['service_code', 'prefix_type', 'hierarchy_idx', 
                  'global_idx', 'description']].to_csv(
            output_path, 
            index=False,
            encoding='utf-8'
        )
        
        print(f"‚úÖ CSV —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(output_df)} —Å—Ç—Ä–æ–∫")
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        debug_path = output_path.replace('.csv', '_debug.csv')
        output_df.to_csv(debug_path, index=False, encoding='utf-8')
        print(f"üìÑ –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {debug_path}")
    
    def save_vocabularies(self, output_dir: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –≤ JSON —Ñ–∞–π–ª—ã"""
        print(f"\nüìö –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π –≤ {output_dir}")
        
        os.makedirs(output_dir, exist_ok=True)
        
        for vocab_name, vocab_dict in self.vocabs.items():
            file_path = os.path.join(output_dir, f'service_{vocab_name}_vocab.json')
            
            # –î–ª—è JSON —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (–∫–ª—é—á–∏-—Å—Ç—Ä–æ–∫–∏)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(
                    {str(k): v for k, v in vocab_dict.items()},  # –í—Å–µ –∫–ª—é—á–∏ –≤ —Å—Ç—Ä–æ–∫–∏
                    f, 
                    indent=2, 
                    ensure_ascii=False
                )
            
            print(f"   ‚úÖ {vocab_name}: {len(vocab_dict)} –∑–∞–ø–∏—Å–µ–π ‚Üí {file_path}")
        
        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ (idx ‚Üí value) –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        reverse_vocabs = {}
        for vocab_name, vocab_dict in self.vocabs.items():
            reverse_dict = {v: k for k, v in vocab_dict.items()}
            reverse_vocabs[vocab_name] = reverse_dict
            
            file_path = os.path.join(output_dir, f'service_{vocab_name}_reverse.json')
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(
                    {str(k): str(v) for k, v in reverse_dict.items()},
                    f,
                    indent=2,
                    ensure_ascii=False
                )
        
        print("‚úÖ –û–±—Ä–∞—Ç–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
        
        return reverse_vocabs
    
    def analyze_data(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("\nüìä –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• –£–°–õ–£–ì:")
        print("=" * 50)
        
        # 1. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–¥–æ–≤ –ø–æ —Ç–∏–ø–∞–º –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤
        type_counts = {'ds': 0, 'st': 0, 'simple': 0, 'dotted': 0}
        
        for code in self.df['TEXTCODE']:
            prefix_type = self.determine_prefix_type(code)
            if prefix_type in type_counts:
                type_counts[prefix_type] += 1
        
        print("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–æ–≤ –ø–æ —Ç–∏–ø–∞–º:")
        for prefix_type, count in type_counts.items():
            if count > 0:
                print(f"   {prefix_type}: {count:4d} –∫–æ–¥–æ–≤")
        
        # 2. –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
        print(f"\nüîç –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞:")
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
        examples = {'ds': [], 'st': [], 'simple': [], 'dotted': []}
        
        for code in self.df['TEXTCODE']:
            prefix_type = self.determine_prefix_type(code)
            if len(examples[prefix_type]) < 3:
                examples[prefix_type].append(code)
        
        for prefix_type, codes_list in examples.items():
            if codes_list:
                print(f"   {prefix_type}: {', '.join(codes_list[:3])}")
        
        # 3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–º –∫–æ–¥–∞–º (A)
        a_counts = self.df['A'].value_counts().head(10)
        print(f"\nüèÜ –¢–æ–ø-10 —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø (A):")
        for a, count in a_counts.items():
            if pd.notna(a):
                print(f"   A={a}: {count} –∫–æ–¥–æ–≤")
        
        print("=" * 50)
    
    def decode_prefix_type(self, prefix_idx: int) -> str:
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–∏–ø –ø—Ä–µ—Ñ–∏–∫—Å–∞"""
        reverse_mapping = {
            2: 'ds',
            3: 'st',
            4: 'simple',
            5: 'dotted',
            0: '<PAD>',
            1: '<UNK>'
        }
        return reverse_mapping.get(prefix_idx, '<UNK>')

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    current_path = Path(__file__).parent  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è file.py
    xlsx_path = current_path / '..' / '..' / '..' / 'res' / 'datasets' / 'codeUsl.xlsx'  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    xlsx_path = xlsx_path.resolve()
    
    print(f"–ü—É—Ç—å –∫ XLSX: {xlsx_path}")
    
    if not xlsx_path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {xlsx_path}")
        print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —É—Å–ª—É–≥ –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä
    converter = ServiceXLSXConverter(xlsx_path)
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞
        converter.load_xlsx()
        
        # 2. –û—á–∏—Å—Ç–∫–∞
        converter.clean_and_prepare()
        
        # 3. –ê–Ω–∞–ª–∏–∑
        converter.analyze_data()
        
        # 4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π
        vocabs = converter.build_vocabularies()
        
        # 5. –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ DataFrame
        output_df = converter.create_output_dataframe()
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV
        save_file_path = current_path / '..' / '..' / '..' / 'res' / 'datasets' / 'services_handbook.csv'
        #converter.save_output_csv(output_df, save_file_path)
        
        print(f"\nüéâ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÅ –í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print(f"   ‚Ä¢ CSV —Å –∫–æ–¥–∞–º–∏: {save_file_path}")
        print(f"   ‚Ä¢ –°–ª–æ–≤–∞—Ä–∏: {vocab_dir}/*.json")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤—ã—Ö 10 –∑–∞–ø–∏—Å–µ–π:")
        for i, row in output_df.head(10).iterrows():
            prefix_type = converter.decode_prefix_type(row['prefix_type'])
            print(f"   {row['service_code']} -> —Ç–∏–ø: {prefix_type}({row['prefix_type']}), "
                  f"–∏–µ—Ä–∞—Ä—Ö–∏—è: {row['hierarchy_idx']}, global: {row['global_idx']}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()



main()